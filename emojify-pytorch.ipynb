{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emojify_data.csv tesss.csv        train_emoji.csv\n",
      "glove.6B.50d.txt test_emoji.csv\n",
      "never talk to me again,3,,\n",
      "I am proud of your achievements,2,,\n",
      "It is the worst day in my life,3,,\n",
      "Miss you so much,0,, [0]\n",
      "food is life,4,,\n",
      "I love you mum,0,,\n",
      "Stop saying bullshit,3,,\n",
      "congratulations on your acceptance,2,,\n",
      "The assignment is too long ,3,,\n",
      "I want to go play,1,, [3]\n"
     ]
    }
   ],
   "source": [
    "!ls data \n",
    "!head -n 10 data/train_emoji.csv\n",
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    print(emb_matrix.shape)\n",
    "    print(len(word_to_index.items()))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            print(word)\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1a1a79f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojisDataset(Data.Dataset):\n",
    "    \"\"\"Emojiss dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, word_to_index, transform=None, one_hot=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        sentences, labels = read_csv(csv_file)\n",
    "        max_len = len(max(sentences, key=len).split())\n",
    "        print(\"max_len = {}\".format(max_len))\n",
    "        self.sentences = sentences_to_indices(sentences, word_to_index, max_len)\n",
    "        self.labels = convert_to_one_hot(labels, C = 5) if one_hot else labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences) if self.sentences is not None else 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.sentences[index], self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 10\n"
     ]
    }
   ],
   "source": [
    "train_data = EmojisDataset(\n",
    "    csv_file='data/train_emoji.csv',\n",
    "    word_to_index=word_to_index,\n",
    ")\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=32, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 8\n"
     ]
    }
   ],
   "source": [
    "test_data = EmojisDataset(\n",
    "    csv_file='data/tesss.csv',\n",
    "    word_to_index=word_to_index,\n",
    ")\n",
    "test_loader = Data.DataLoader(dataset=train_data, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    embedding_matrix = np.zeros((vocab_len, embedding_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        embedding_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "    weights = torch.FloatTensor(embedding_matrix)\n",
    "    print(\"len(weights) = \", len(weights))\n",
    "    embedding_layer = nn.Embedding.from_pretrained(weights)\n",
    "    return embedding_layer, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojifyNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_to_vec_map, word_to_index, hidden_size, num_layers, batch_size, out_features):\n",
    "        super(EmojifyNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.embedding, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=hidden_size, num_layers=self.num_layers, dropout=0.5)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, 10, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, 10, self.hidden_size))\n",
    "        \n",
    "    def forward(self, sentence, hidden):\n",
    "        embeds = self.embedding(sentence)\n",
    "#         print(\"Embedding shape = {}\".format(embeds.shape))\n",
    "        lstm_out, _ = self.lstm(embeds, hidden)\n",
    "#         print(\"LSTM shape = {}\".format(lstm_out.shape))\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        linear_out = self.linear(lstm_out)\n",
    "#         print(\"Linear shape = {}\".format(linear_out.shape))\n",
    "        out = self.softmax(linear_out)\n",
    "#         print(\"Softmax shape = {}\\n\".format(out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(weights) =  400001\n"
     ]
    }
   ],
   "source": [
    "model = EmojifyNN(word_to_vec_map, word_to_index, hidden_size=128, num_layers=2, batch_size=32, out_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer1 = torch.optim.Adam(model.lstm.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(model.linear.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(80):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for step, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long()\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        outputs = model(inputs, hidden)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "#         if (epoch+1) % 5 == 0:\n",
    "#             for step, data in enumerate(train_loader):\n",
    "#                 test_sentences, test_labels = data\n",
    "#                 test_sentences = test_sentences.long()\n",
    "#                 test_labels = test_labels.long()\n",
    "# #                 x_test = test_sentences # np.array(['not feeling happy', 'Holy shit', 'you are so pretty', 'let us play ball'])\n",
    "# #                 X_test_indices = sentences_to_indices(x_test, word_to_index, 32)\n",
    "# #                 X_test_indices = torch.from_numpy(X_test_indices)\n",
    "# #                 X_test_indices = X_test_indices.long()\n",
    "#                 hidden = model.init_hidden()\n",
    "#                 prediction = model(test_sentences, hidden)\n",
    "#                 for i in range(len(test_sentences)):\n",
    "#                     pred = prediction.data.numpy()\n",
    "#                     num = np.argmax(pred[i])\n",
    "#                     print(' prediction: ', test_sentences[i], label_to_emoji(num))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 25 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        sentences, labels = data\n",
    "        sentences = sentences.long()\n",
    "        labels = labels.long()\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = model.init_hidden()\n",
    "        outputs = model(sentences, hidden)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Accuracy of the network: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
